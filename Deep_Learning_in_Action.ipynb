{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, _nbpresent_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nbpresent\n",
    "nbpresent.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of AI\n",
    "\n",
    "> In the early days of artificial intelligence, the field rapidly tackled and solved problems that are intellectually difficult for human beings but relatively straightforward for computers - problems that can be described by a list of formal, mathematical rules. The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally—problems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy for us. Difficult for computers\n",
    "\n",
    "- object recognition\n",
    "- speech recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations matter\n",
    "\n",
    "<figure>\n",
    "    <img src='coords.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just feed the network the right features?\n",
    "\n",
    "- What are the correct pixel values for a \"bike\" feature?\n",
    "  - race bike, mountain bike, e-bike?\n",
    "  - pixels in the shadow may be _much_ darker\n",
    "  - what if bike is mostly obscured by rider standing in front?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the network pick the features\n",
    "\n",
    "### a layer at a time\n",
    "\n",
    "<figure>\n",
    "    <img src='features.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning, 2 ways to think about it\n",
    "\n",
    "- hierarchical feature extraction (start simple, end complex)\n",
    "- function composition (see http://colah.github.io/posts/2015-09-NN-Types-FP/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short History of (Deep) Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first wave: cybernetics (1940s - 1960s)\n",
    "\n",
    "- neuroscientific motivation\n",
    "- linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch-Pitts Neuron (MLP, 1943, a.k.a. Logic Circuit)\n",
    "\n",
    "- binary output (0 or 1)\n",
    "- neurons may have inhibiting (negative) and excitatory (positive) inputs\n",
    "- each neuron has a threshold that has to be surpassed by the sum of activations for the neuron to get active (output 1)\n",
    "- if just one input is inhibitory, the neuron will not activate\n",
    "\n",
    "<figure>\n",
    "    <img src='mcp.png' alt='missing' />\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (Rosenblatt, 1958): the great hope\n",
    "\n",
    "- compute linear combination of inputs\n",
    "- return \n",
    "\n",
    "<figure>\n",
    "    <img src='perceptron.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minsky & Papert (1969), \"Perceptrons\": the great disappointment\n",
    "\n",
    "- Perceptrons can only solve linearly separable problems\n",
    "- Big loss of interest in neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second wave: Connectionism (1980s, mid-1990s)\n",
    "\n",
    "- distributed representations\n",
    "- popularization of backpropagation (Rumelhart\n",
    "et al., 1986a ; LeCun , 1987)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The magic ingredient: backpropagation\n",
    "\n",
    "- Bryson, A.E.; W.F. Denham; S.E. Dreyfus.  Optimal programming problems with inequality constraints.  I:\n",
    "Necessary conditions for extremal solutions.  AIAA J. 1, 11 (1963) 2544-2550\n",
    ")\n",
    "- Paul Werbos, David E. Rumelhart, Geo rey E. Hinton 1974\n",
    "- Ronald J. Williams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop: How could the magic fail?\n",
    "\n",
    "- Only applicable in case of supervised learning\n",
    "- Doesn't scale well to multiple layers\n",
    "- Can converge to poor local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop: the return\n",
    "\n",
    "- much increased computing power because of GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third wave: Deep Learning\n",
    "\n",
    "- everything starts with Hinton 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithms en vogue now  have mostly been around since the 1980s/1990s.\n",
    "\n",
    "- convolutional neural networks: \n",
    "- recurrent neural networks:\n",
    "- LSTM\n",
    "\n",
    "### So why the hype / success NOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data\n",
    "\n",
    "> It is true\n",
    "that some skill is required to get good performance from a deep learning algorithm.\n",
    "Fortunately, the amount of skill required reduces as the amount of training data\n",
    "increases. The learning algorithms reaching human performance on complex tasks\n",
    "today are nearly identical to the learning algorithms that struggled to solve toy\n",
    "problems in the 1980s, though the models we train with these algorithms have\n",
    "undergone changes that simplify the training of very deep architectures.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size - rule of thumb\n",
    "\n",
    "> As of 2016, a rough rule of thumb\n",
    "is that a supervised deep learning algorithm will generally achieve acceptable\n",
    "performance with around 5,000 labeled examples per category, and will match or\n",
    "exceed human performance when trained with a dataset containing at least 10\n",
    "million labeled examples.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big models\n",
    "\n",
    "thanks to faster/better\n",
    "- hardware (CPUs, GPUs)\n",
    "- network infrastructure\n",
    "- software implementations\n",
    "\n",
    "> Since the introduction of hidden units, artificial neural networks have doubled in size roughly every 2.4 years.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big impact\n",
    "\n",
    "- deep networks consistently wins prestigious competitions (e.g., ImageNet)\n",
    "- deep learning solves increasingly complex problems (e.g., sequence-to-sequence learning)\n",
    "- deep learning has started to fuel _other research areas_ \n",
    "\n",
    "and most importantly: Deep learning is _highly profitable_\n",
    "\n",
    "> Deep learning is now used by many top technology companies including Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netflix, NVIDIA and NEC.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Deep Neural Network\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src='deep_nn.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat (terminology-related)\n",
    "\n",
    ">So “multi-layer” neural networks do not use the perceptron learning\n",
    "procedure.\n",
    "\n",
    ">They should never have been called multi-layer perceptrons.\n",
    "\n",
    "Geoffrey Hinton, [Neural Networks for Machine Learning Lec. 3](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec3.pdf)\n",
    "\n",
    "#### What people mean by MLP is just a _deep_ _feedforward_ neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR\n",
    "\n",
    "We want to predict\n",
    "- 0 from [0,0]\n",
    "- 0 from [1,1]\n",
    "- 1 from [0,1]\n",
    "- 1 from [1,0]\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src='xor1.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a linear model \n",
    "\n",
    "$f(\\mathbf{x}; \\mathbf{w}, b) = \\mathbf{x}^T\\mathbf{w} + b$    \n",
    "\n",
    "(x = vector)\n",
    "\n",
    "- with MSE cost, this leads to: $\\mathbf{w}=0, b=0.5$\n",
    "- mapping every point to 0.5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce hidden layer with nonlinear activation function\n",
    "\n",
    "$f(\\mathbf{x}; \\mathbf{W}, \\mathbf{c}, \\mathbf{w}, b) = \\mathbf{w}^T max(0, \\mathbf{W}^T\\mathbf{x} + \\mathbf{c}) + b$\n",
    "\n",
    "(x = vector)\n",
    "\n",
    "<figure>\n",
    "    <img src='xor4.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation with hidden layer\n",
    "\n",
    "\n",
    "- Design matrix: $\\mathbf{X} = \\begin{bmatrix}0 & 0 \\\\0 & 1\\\\1 & 0 \\\\1 & 1\\end{bmatrix}$\n",
    "\n",
    "- Parameters: $\\mathbf{W} = \\begin{bmatrix}1 & 1 \\\\1 & 1\\end{bmatrix}$, $\\mathbf{c} = \\begin{bmatrix}0 \\\\ -1 \\end{bmatrix}$, $\\mathbf{w} = \\begin{bmatrix}1 \\\\ -2 \\end{bmatrix}$\n",
    "\n",
    "- Input to hidden layer: $\\mathbf{X}\\mathbf{W} = \\begin{bmatrix}0 & 0 \\\\1 & 1\\\\1 & 1 \\\\2 & 2\\end{bmatrix}$, add $\\mathbf{c}$ to every row ==> $\\begin{bmatrix}0 & -1 \\\\1 & 0\\\\1 & 0 \\\\2 & 1\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which gives us...\n",
    "\n",
    "<figure>\n",
    "    <img src='xor2.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing nonlinearity\n",
    "\n",
    "Output of rectified linear transformation:  $\\begin{bmatrix}0 & 0 \\\\1 & 0\\\\1 & 0 \\\\2 & 1\\end{bmatrix}$\n",
    "<figure>\n",
    "    <img src='xor3.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "\n",
    "The remaining hidden-to-output transformation is linear, but the classes are already linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train a deep network (1): Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optimization\n",
    "\n",
    "- Like other machine learning algorithms, neural networks learn by _minimizing a cost function_.\n",
    "- Cost functions in neural networks normally are not convex and so, cannot be optimized in closed form.\n",
    "- The solution is to do gradient descent.\n",
    "\n",
    "<figure>\n",
    "    <img src='convex.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local minima\n",
    "\n",
    "<figure>\n",
    "    <img src='local_minima.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form vs. gradient descent optimization by example: Least Squares\n",
    "\n",
    "- Minimize squared error $f(\\mathbf{x}) = ||\\mathbf{X\\hat{\\beta} - y}||^2_2$\n",
    "- Closed form: solve __normal equations__ $\\mathbf{\\hat{\\beta}} = (\\mathbf{XX}^T)^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- Or follow the gradient: $\\nabla_x f(\\mathbf{x})= \\mathbf{X}^T\\mathbf{X}\\mathbf{\\hat{\\beta}} - \\mathbf{X^Ty}$\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='gradient_descent_LS.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This gives us a way to train one weight matrix. How about a net with several layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train a deep network (2): Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who else to ask but Geoff Hinton...\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<figure>\n",
    "    <img src='backprop1.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Geoffrey Hinton, [Neural Networks for Machine Learning Lec. 3](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec3.pdf)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mechanics of backprop\n",
    "\n",
    "- basically, just the chain rule: $\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}$\n",
    "- chained over several layers:\n",
    "&nbsp;\n",
    "<figure>\n",
    "    <img src='backprop2.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: https://colah.github.io/posts/2015-08-Backprop/</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop example: logistic neuron\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<figure>\n",
    "    <img src='backprop3.png' alt='missing' width='60%' align='left'/>\n",
    "    <figcaption>Source: Geoffrey Hinton, [Neural Networks for Machine Learning Lec. 3](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec3.pdf)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decisions (1): Which loss function should I choose?\n",
    "\n",
    "- the _loss_ (or _cost_) function indicates the cost incurred from false prediction / misclassification\n",
    "\n",
    "- probably the best-known loss function in machine learning is __mean squared error__: \n",
    "\n",
    "  $\\frac{1}{n} \\sum_n{(\\hat{y} - y)^2}$\n",
    "  \n",
    "- most of the time, in deep learning we use __cross entropy__:\n",
    "\n",
    "  $- \\sum_j{t_j log(y_j)}$\n",
    "  \n",
    "  This is the negative log probability of the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions (2): Which activation function to choose?\n",
    "\n",
    "- purpose of activation function: introduce nonlinearity (see above)\n",
    "- for a long time, the sigmoid (logistic) activation function was used a lot:\n",
    "\n",
    "  $y = \\frac{1}{1 + e^{-z}}$\n",
    "  \n",
    "  <figure>\n",
    "    <img src='sigmoid.png' alt='missing' width='40%'/>\n",
    "  </figure>\n",
    "  \n",
    "  \n",
    "- now _rectified linear units_ (ReLUs) are preferred:\n",
    "\n",
    "  $y = max(0, z)$\n",
    "  \n",
    "    <figure>\n",
    "    <img src='ReLU.png' alt='missing' width='40%'/>\n",
    "  </figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5161136-7f9f-4a01-b48d-eb18a86d3cb3"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Why Conv Nets?\n",
    "\n",
    "- conventional feedforward networks need equally sized input (images for example normally aren't!)\n",
    "- convolution operation extracts image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='convnet.jpeg' alt='missing' />\n",
    "    <figcaption>Source: http://cs231n.github.io/convolutional-networks/</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='convolution_demo.png' alt='missing' align='left' width='60%'/>\n",
    "    <figcaption>Source: http://cs231n.github.io/convolutional-networks/ (Live Demo on website!)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and cross-correlation\n",
    "\n",
    "- Strictly, the operation shown above (and implemented in most DL libraries) is not convolution, but cross-correlation\n",
    "- 1d discrete convolution: $s(t) = (x * w)(t) = \\sum_a{x(a) w(t-a)}$\n",
    "- 2d convolution: $S(i,j) = I * K (i,j) = \\sum_m \\sum_n{I(m,n)K(i-m,j-n)}$\n",
    "- 2d cross-correlation:  $S(i,j) = I * K (i,j) = \\sum_m \\sum_n{I(i+m,j+n)K(m,n)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Octave demo\n",
    "\n",
    "```\n",
    "A = [1,2,3;4,5,6;7,8,9] # input \"image\"\n",
    "# padded input matrix, for easier visualization\n",
    "A_padded = [zeros(1,size(A,2)+2); [zeros(size(A,1),1), A, zeros(size(A,1),1)]; zeros(1,size(A,2)+2)]\n",
    "B = [1,0;0,0] # kernel\n",
    "\n",
    "# real convolution\n",
    "C_full = conv2(A,B,'full') # default\n",
    "C_same = conv2(A,B,'same') \n",
    "C_valid = conv2(A,B,'valid')\n",
    "\n",
    "# cross-correlation\n",
    "XC = xcorr2(A,B) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gimp demo\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- Edge enhance: $\\begin{bmatrix}0 & 0 & 0\\\\-1 & 1 & 0\\\\0 & 0 & 0\\end{bmatrix}$, edge detect: $\\begin{bmatrix}0 & 1 & 0\\\\1 & -4 & 1\\\\0 & 1 & 0\\end{bmatrix}$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- Blur: $\\begin{bmatrix}1 & 1 & 1\\\\1 & 1 & 1\\\\1 & 1 & 1\\end{bmatrix}$, sharpen: $\\begin{bmatrix}0 & -1 & 0\\\\-1 & 5 & -1\\\\0 & -1 & 0\\end{bmatrix}$\n",
    "\n",
    "\n",
    "https://docs.gimp.org/en/plug-in-convmatrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Modeling\n",
    "\n",
    "- predict next word given preceding ones\n",
    "- based on statistical properties of the distribution of sequences of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional hypothesis: linguistic items with similar distributions have similar meanings\n",
    "\n",
    "- n-gram/count-based (e.g., LSA)\n",
    "- predictive (neural network language models, e.g., word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram-based\n",
    "\n",
    "- choose ngram-size n\n",
    "- estimate the probability $P(w_{t+1}|w_1,...,w_{t−2},w_{t−1},w_t)$ by ignoring context beyond n−1 words and dividing by the count of all given words up till $\\mathbf{w_t}$\n",
    "- e.g., with bigrams: $P(w_{t+1}|w_t = \\frac{count(w_{t+1},w_t)}{count(w_t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network example (Bengio et al 2001, Bengio et al 2003)\n",
    "\n",
    "- choose a context size n, as in ngrams\n",
    "- map each word $w_{t−i}$ in the n−1-word context to an associated d-dimensional feature vector $C_{w_{t-i}}$\n",
    "- predict next word using standard NN architecture with tanh (hidden layer) resp. softmax (output layer) activation functions \n",
    "- train network to maximize log likelihood $L(θ)=\\sum_t{log P(w_t|w_{t−n+1}, ..., w_{t−1}})$ using stochastic gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings: word2vec\n",
    "\n",
    "Mikolov et al (2013a).  Efficient estimation of word representations in vector space. arXiv:1301.3781.\n",
    "\n",
    "- Continuous Bag of Words (CBOW)\n",
    "- Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Continuous Bag of Words \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='cbow.png' alt='missing' align='left'/>\n",
    "    <figcaption>Source: Mikolov et al. 2013, Efficient estimation of word representations in vector space. arXiv:1301.3781.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Skip-gram\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='skip_gram.png' alt='missing' align='left'/>\n",
    "    <figcaption>Source: Mikolov et al. 2013, Efficient estimation of word representations in vector space. arXiv:1301.3781.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Semantic & syntactic relationships\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='relationships.png' alt='missing' align='left' />\n",
    "    <figcaption>Source: Mikolov et al. 2013, Efficient estimation of word representations in vector space. arXiv:1301.3781.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec visualizations\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='relations2.png' alt='missing' align='left' width='60%'/>\n",
    "    <figcaption>Source: https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why have recursion ?\n",
    "\n",
    "- cannot process sequential data with \"normal\" feedforward networks\n",
    "- in NLP, the n-gram approach cannot handle long-term relationships\n",
    "\n",
    "> Jane walked into the room. John walked in too. It was late in the\n",
    "day, and everyone was walking home after a long day at work. Jane said hi to ___ (\n",
    "[Stanford CS 224D Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Two representations of RNNs\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='rnn1.png' alt='missing' align='left' width='60%'/>\n",
    "    <figcaption>Source: Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The recursion: example code\n",
    "\n",
    "```\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)\n",
    "\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state) \n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]\n",
    "```\n",
    "from:<a href='http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html'>http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNNs in practice: The need to forget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRUs)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='gru.png' alt='missing' align='left' width='60%'/> <img src='gru2.png' alt='missing' align='right' width='40%'/>\n",
    "    <figcaption>Source: [Stanford CS 224D Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='lstm.png' alt='missing' align='left' width='60%'/> <img src='lstm2.png' alt='missing' align='right' width='40%'/>\n",
    "    <figcaption>Source: [Stanford CS 224D Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU vs. LSTM\n",
    "\n",
    "<table>\n",
    "<tr><th></th><th>LSTM</th><th>GRU</th></tr>\n",
    "<tr><th>reset/forget gate</th><td>how much of hidden state to keep? (as opposed to new input)</td><td>how much of hidden state to keep? (as opposed to new input)</td></tr>\n",
    "<tr><th>input gate</th><td>how much to keep of new input?</td><td>n.a.</td></tr>\n",
    "<tr><th>update gate</th><td>n.a.</td><td>how much of hidden state to keep? (as opposed to new memory)</td></tr>\n",
    "<tr><th>new memory</th><td>combines new input and old state (gated by input gate and forget gate)</td><td>combines new input and old state (gated by forget gate)</td></tr>\n",
    "<tr><th>hidden state</th><td>combines new memory (gated by input gate) and old state (gated by forget gate)</td><td>combines new memory and old state (gated by update gate)</td></tr>\n",
    "<tr><th>hidden state 2</th><td>new hidden state, gated by output gate</td><td>n.a.</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU vs. LSTM: code example (Tensorflow)\n",
    "### Baseline: Simple RNN Cell\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "```\n",
    "class BasicRNNCell(RNNCell):\n",
    "  \"\"\"The most basic RNN cell.\"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "   <...>\n",
    "   \n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "    with vs.variable_scope(scope or \"basic_rnn_cell\"):\n",
    "      output = self._activation(\n",
    "          _linear([inputs, state], self._num_units, True, scope=scope))\n",
    "return output, output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU vs. LSTM: code example (Tensorflow)\n",
    "### GRU Cell\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "```\n",
    "class GRUCell(RNNCell):\n",
    "  def __init__(self, num_units, input_size=None, activation=tanh): <...>\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with vs.variable_scope(scope or \"gru_cell\"):\n",
    "      with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "        # We start with bias of 1.0 to not reset and not update.\n",
    "        r, u = array_ops.split(\n",
    "            1, 2, _linear([inputs, state], 2 * self._num_units, True, 1.0, scope=scope))\n",
    "        r, u = sigmoid(r), sigmoid(u)\n",
    "      with vs.variable_scope(\"candidate\"):\n",
    "        c = self._activation(_linear([inputs, r * state], self._num_units, True, scope=scope))\n",
    "      new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU vs. LSTM: code example (Tensorflow)\n",
    "### LSTM Cell\n",
    "&nbsp;\n",
    "\n",
    "```\n",
    "class BasicLSTMCell(RNNCell):\n",
    "  def __init__(self, num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=tanh): <...>\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with vs.variable_scope(scope or \"basic_lstm_cell\"):\n",
    "      # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "      if self._state_is_tuple:\n",
    "        c, h = state\n",
    "      else:\n",
    "        c, h = array_ops.split(1, 2, state)\n",
    "      concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      i, j, f, o = array_ops.split(1, 4, concat)\n",
    "\n",
    "      new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n",
    "      new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "      if self._state_is_tuple:\n",
    "        new_state = LSTMStateTuple(new_c, new_h)\n",
    "      else:\n",
    "        new_state = array_ops.concat(1, [new_c, new_h])\n",
    "      return new_h, new_state\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping sequences to sequences: seq2seq\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "    <img src='seq2seq.png' alt='missing' align='left' width='100%'/> \n",
    "    <figcaption>Source: [Tensorflow seq2seq tutorial](https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html)</figcaption>\n",
    "</figure>\n",
    "\n",
    "- first RNN encodes the input, second decodes the output\n",
    "- applications: e.g., machine translation - though basically, all sequence-to-sequence translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining modes/models example: Images and Descriptions\n",
    "\n",
    "- [Andrej Karpathy, Li Fei-Fei: Deep Visual-Semantic Alignments for Generating Image Descriptions](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)\n",
    "- combining CNNs, bidirectional RNNs, and multimodal embeddings\n",
    "- [Demo](http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/)\n",
    "\n",
    "<figure>\n",
    "    <img src='strawhat.png' alt='missing' align='left' width='50%'/> \n",
    "    <figcaption>Source: [Deep Visual-Semantic Alignments for Generating Image Descriptions\n",
    "](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "8265f3a6-a294-47f6-bc75-69bafdcf8e5f": {
     "id": "8265f3a6-a294-47f6-bc75-69bafdcf8e5f",
     "prev": "8916f865-3a24-4393-bbca-6ca15f3d0de4",
     "regions": {
      "5487a8aa-a437-4e14-952d-9f4e79de0da9": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "1f3634d5-a141-48bb-a685-6d3c7e003c2b",
        "part": "source"
       },
       "id": "5487a8aa-a437-4e14-952d-9f4e79de0da9"
      }
     }
    },
    "8916f865-3a24-4393-bbca-6ca15f3d0de4": {
     "id": "8916f865-3a24-4393-bbca-6ca15f3d0de4",
     "prev": null,
     "regions": {
      "47c14712-510f-4e1b-8725-0af0ec79ac7e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "47c14712-510f-4e1b-8725-0af0ec79ac7e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
