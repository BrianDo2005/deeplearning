{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, _nbpresent_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nbpresent\n",
    "nbpresent.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the early days of artificial intelligence, the field rapidly tackled and solved\n",
    "problems that are intellectually difficult for human beings but relatively straight-\n",
    "forward for computers—problems that can be described by a list of formal, math-\n",
    "ematical rules. The true challenge to artificial intelligence proved to be solving\n",
    "the tasks that are easy for people to perform but hard for people to describe\n",
    "formally—problems that we solve intuitively, that feel automatic, like recognizing\n",
    "spoken words or faces in images.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy for us. Difficult for computers\n",
    "\n",
    "- object recognition\n",
    "- speech recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations matter\n",
    "\n",
    "<figure>\n",
    "    <img src='coords.png' alt='missing' />\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just feed the network the right features?\n",
    "\n",
    "- What are the correct pixel values for a \"bike\" feature?\n",
    "  - race bike, mountain bike, e-bike?\n",
    "  - pixels in the shadow may be _much_ darker\n",
    "  - what if bike is mostly obscured by rider standing in front?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the network pick the features\n",
    "\n",
    "### a layer at a time\n",
    "\n",
    "<figure>\n",
    "    <img src='features.png' alt='missing' />\n",
    "    <figcaption>Source: hGoodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning, 2 ways to think about it\n",
    "\n",
    "- hierarchical feature extraction (start simple, end complex)\n",
    "- function composition (see http://colah.github.io/posts/2015-09-NN-Types-FP/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short History of (Deep) Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first wave: cybernetics (1940s - 1960s)\n",
    "\n",
    "- neuroscientific motivation\n",
    "- linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch-Pitts Neuron (MLP, 1943, a.k.a. Logic Circuit)\n",
    "\n",
    "- binary output (0 or 1)\n",
    "- neurons may have inhibiting (negative) and excitatory (positive) inputs\n",
    "- each neuron has a threshold that has to be surpassed by the sum of activations for the neuron to get active (output 1)\n",
    "- if just one input is inhibitory, the neuron will not activate\n",
    "\n",
    "<figure>\n",
    "    <img src='mcp.png' alt='missing' />\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (Rosenblatt, 1958): the great hope\n",
    "\n",
    "- compute linear combination of inputs\n",
    "- return \n",
    "\n",
    "<figure>\n",
    "    <img src='perceptron.png' alt='missing' />\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minsky & Papert (1969), \"Perceptrons\": the great disappointment\n",
    "\n",
    "- Perceptrons can only solve linearly separable problems\n",
    "- Big loss of interest in neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second wave: Connectionism (1980s, mid-1990s)\n",
    "\n",
    "- distributed representations\n",
    "- popularization of backpropagation (Rumelhart\n",
    "et al., 1986a ; LeCun , 1987)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The magic ingredient: backpropagation\n",
    "\n",
    "- Bryson, A.E.; W.F. Denham; S.E. Dreyfus.  Optimal programming problems with inequality constraints.  I:\n",
    "Necessary conditions for extremal solutions.  AIAA J. 1, 11 (1963) 2544-2550\n",
    ")\n",
    "- Paul Werbos, David E. Rumelhart, Geo rey E. Hinton 1974\n",
    "- Ronald J. Williams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop: How could the magic fail?\n",
    "\n",
    "- Only applicable in case of supervised learning\n",
    "- Doesn't scale well to multiple layers\n",
    "- Can converge to poor local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop: the return\n",
    "\n",
    "- much increased computing power because of GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The third wave: Deep Learning\n",
    "\n",
    "- everything starts with Hinton 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithms en vogue now  have mostly been around since the 1980s/1990s.\n",
    "\n",
    "- convolutional neural networks: \n",
    "- recurrent neural networks:\n",
    "- LSTM\n",
    "\n",
    "### So why the hype / success NOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data\n",
    "\n",
    "> It is true\n",
    "that some skill is required to get good performance from a deep learning algorithm.\n",
    "Fortunately, the amount of skill required reduces as the amount of training data\n",
    "increases. The learning algorithms reaching human performance on complex tasks\n",
    "today are nearly identical to the learning algorithms that struggled to solve toy\n",
    "problems in the 1980s, though the models we train with these algorithms have\n",
    "undergone changes that simplify the training of very deep architectures.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule of thumb\n",
    "\n",
    "> As of 2016, a rough rule of thumb\n",
    "is that a supervised deep learning algorithm will generally achieve acceptable\n",
    "performance with around 5,000 labeled examples per category, and will match or\n",
    "exceed human performance when trained with a dataset containing at least 10\n",
    "million labeled examples.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big models\n",
    "\n",
    "thanks to faster/better\n",
    "- hardware (CPUs, GPUs)\n",
    "- network infrastructure\n",
    "- software implementations\n",
    "\n",
    "> Since the introduction of hidden units, artificial neural networks have doubled in size roughly every 2.4 years.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big impact\n",
    "\n",
    "- deep networks consistently wins prestigious competitions (e.g., ImageNet)\n",
    "- deep learning solves increasingly complex problems (e.g., sequence-to-sequence learning)\n",
    "- deep learning has started to fuel _other research areas_ \n",
    "\n",
    "and most importantly: Deep learning is _highly profitable_\n",
    "\n",
    "> Deep learning is now used by many top technology companies including Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netflix, NVIDIA and NEC.\n",
    "\n",
    "Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Deep Neural Network\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src='deep_nn.png' alt='missing' />\n",
    "    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "\n",
    "a.k.a. [deep] feedforward neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR\n",
    "\n",
    "We want to predict\n",
    "- 0 from [0,0]\n",
    "- 0 from [1,1]\n",
    "- 1 from [0,1]\n",
    "- 1 from [1,0]\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src='xor1.png' alt='missing' width=70% />\n",
    "    <figcaption></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a linear model \n",
    "\n",
    "$f(\\mathbf{x}; \\mathbf{w}, b) = \\mathbf{x}^T\\mathbf{w} + b$\n",
    "\n",
    "(x = vector)\n",
    "\n",
    "- with MSE cost, this leads to: $\\mathbf{w}=0, b=0.5$\n",
    "- mapping every point to 0.5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce hidden layer with nonlinear activation function\n",
    "\n",
    "$f(\\mathbf{x}; \\mathbf{W}, \\mathbf{c}, \\mathbf{w}, b) = \\mathbf{w}^T max(0, \\mathbf{W}^T\\mathbf{x} + \\mathbf{c}) + b$\n",
    "\n",
    "(x = vector)\n",
    "\n",
    "<figure>\n",
    "    <img src='xor4.png' alt='missing' />\n",
    "    <figcaption>Source: Goodfellow et al. 2016, [Deep Learning](http://www.deeplearningbook.org/)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation with hidden layer\n",
    "\n",
    "\n",
    "- Design matrix: $\\mathbf{X} = \\begin{bmatrix}0 & 0 \\\\0 & 1\\\\1 & 0 \\\\1 & 1\\end{bmatrix}$\n",
    "\n",
    "- Parameters: $\\mathbf{W} = \\begin{bmatrix}1 & 1 \\\\1 & 1\\end{bmatrix}$, $\\mathbf{c} = \\begin{bmatrix}0 \\\\ -1 \\end{bmatrix}$, $\\mathbf{w} = \\begin{bmatrix}1 \\\\ -2 \\end{bmatrix}$\n",
    "\n",
    "- Input to hidden layer: $\\mathbf{X}\\mathbf{W} = \\begin{bmatrix}0 & 0 \\\\1 & 1\\\\1 & 1 \\\\2 & 2\\end{bmatrix}$, add $\\mathbf{c}$ to every row ==> $\\begin{bmatrix}0 & -1 \\\\1 & 0\\\\1 & 0 \\\\2 & 1\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which gives us...\n",
    "\n",
    "<figure>\n",
    "    <img src='xor2.png' alt='missing' width=70% />\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing nonlinearity\n",
    "\n",
    "<figure>\n",
    "    <img src='xor3.png' alt='missing' width=70% />\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "\n",
    "voilà!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to learn: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to learn: backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward networks: Lessons learned\n",
    "\n",
    "- use the right loss function (cross entropy instead of mean squared error)\n",
    "- use the right activation function (rectified linear instead of sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5161136-7f9f-4a01-b48d-eb18a86d3cb3"
    }
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='convnet.jpeg' alt='missing' />\n",
    "    <figcaption>Source: http://cs231n.github.io/convolutional-networks/</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ConvNets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='convolution_demo.png' alt='missing' />\n",
    "    <figcaption>Source: http://cs231n.github.io/convolutional-networks/ (Live Demo on website!)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution - the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of filters K\n",
    ",\n",
    "their spatial extent F\n",
    ",\n",
    "the stride S\n",
    ",\n",
    "the amount of zero padding P\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Modeling\n",
    "\n",
    "- predict next word given preceding ones\n",
    "- based on statistical properties of the distribution of sequences of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional hypothesis: linguistic items with similar distributions have similar meanings\n",
    "\n",
    "- n-gram/count-based (e.g., LSA)\n",
    "- predictive (neural network language models, e.g., word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram-based\n",
    "\n",
    "- choose ngram-size n\n",
    "- estimate the probability $\\mathbf{P(w_{t+1}|w_1,...,w_{t−2},w_{t−1},w_t)}$ by ignoring context beyond n−1 words and dividing by the count of all given words up till $\\mathbf{w_t}$\n",
    "- e.g., with bigrams: $\\mathbf{P(w_{t+1}|w_t = \\frac{count(w_{t+1},w_t)}{count(w_t)})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network example (Bengio et al 2001, Bengio et al 2003)\n",
    "\n",
    "- choose a context size n, as in ngrams\n",
    "- map each word $\\mathbf{w_{t−i}}$ in the n−1-word context to an associated d-dimensional feature vector $\\mathbf{C_{w_{t-i}}}$\n",
    "- predict next word using standard NN architecture with tanh (hidden layer) resp. softmax (output layer) activation functions \n",
    "- train network to maximize log likelihood $\\mathbf{L(θ)=\\sum_t{log P(w_t|w_{t−n+1}, ..., w_{t−1}})}$ using stochastic gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "8265f3a6-a294-47f6-bc75-69bafdcf8e5f": {
     "id": "8265f3a6-a294-47f6-bc75-69bafdcf8e5f",
     "prev": "8916f865-3a24-4393-bbca-6ca15f3d0de4",
     "regions": {
      "5487a8aa-a437-4e14-952d-9f4e79de0da9": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "1f3634d5-a141-48bb-a685-6d3c7e003c2b",
        "part": "source"
       },
       "id": "5487a8aa-a437-4e14-952d-9f4e79de0da9"
      }
     }
    },
    "8916f865-3a24-4393-bbca-6ca15f3d0de4": {
     "id": "8916f865-3a24-4393-bbca-6ca15f3d0de4",
     "prev": null,
     "regions": {
      "47c14712-510f-4e1b-8725-0af0ec79ac7e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "47c14712-510f-4e1b-8725-0af0ec79ac7e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
