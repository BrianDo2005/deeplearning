{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = 120, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    num_classes,\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 512,\n",
    "    batch_size = 32,\n",
    "    num_steps = 120,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln(tensor, scope = None, epsilon = 1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape()) == 2)\n",
    "    m, v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = ''\n",
    "    with tf.variable_scope(scope + 'layer_norm'):\n",
    "        scale = tf.get_variable('scale',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(1))\n",
    "        shift = tf.get_variable('shift',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(0))\n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "\n",
    "    return LN_initial * scale + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalizedLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicLSTMCell to use Layer Normalization.\n",
    "    Note that state_is_tuple is always True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            c, h = state\n",
    "\n",
    "            # change bias argument to False since LN will add bias via shift\n",
    "            concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "            i, j, f, o = tf.split(1, 4, concat)\n",
    "\n",
    "            # add layer normalization to each gate\n",
    "            i = ln(i, scope = 'i/')\n",
    "            j = ln(j, scope = 'j/')\n",
    "            f = ln(f, scope = 'f/')\n",
    "            o = ln(o, scope = 'o/')\n",
    "\n",
    "            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "            # add layer_normalization in calculation of new hidden state\n",
    "            new_h = self._activation(ln(new_c, scope = 'new_h/')) * tf.nn.sigmoid(o)\n",
    "            new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Zueignung.\n",
      "\n",
      "  Ihr naht euch wieder, schwankende Gestalten,\n",
      "  Die früh sich einst dem trüben Blick \n",
      "{'w', 'F', 'X', ' ', 'K', 'y', '(', 'u', 'I', 'r', 'f', 'V', ',', 'Y', 'h', 'Ö', '!', 'G', 'p', 'C', 'b', 'W', 't', 'Ä', 'O', 'R', ';', '\"', 'E', 'Q', ')', 'c', 'ö', 'N', 'T', 'P', 'd', 'l', 'D', 'o', 'j', 'H', 'g', 'i', 'q', 'B', ':', '.', '\\n', 'A', 'M', 'v', 'Ü', 'L', 'm', 'Z', 'U', '-', 'k', 'ä', 'J', 'S', 'n', 's', '?', 'ü', 'a', 'e', 'ß', 'x', 'z', \"'\"}\n",
      "{0: 'w', 1: 'F', 2: 'X', 3: ' ', 4: 'K', 5: 'y', 6: '(', 7: 'u', 8: 'I', 9: 'r', 10: 'f', 11: 'V', 12: ',', 13: 'Y', 14: 'h', 15: 'Ö', 16: '!', 17: 'G', 18: 'p', 19: 'C', 20: 'b', 21: 'W', 22: 't', 23: 'Ä', 24: 'O', 25: 'R', 26: ';', 27: '\"', 28: 'E', 29: 'Q', 30: ')', 31: 'c', 32: 'ö', 33: 'N', 34: 'T', 35: 'P', 36: 'd', 37: 'l', 38: 'D', 39: 'o', 40: 'j', 41: 'H', 42: 'g', 43: 'i', 44: 'q', 45: 'B', 46: ':', 47: '.', 48: '\\n', 49: 'A', 50: 'M', 51: 'v', 52: 'Ü', 53: 'L', 54: 'm', 55: 'Z', 56: 'U', 57: '-', 58: 'k', 59: 'ä', 60: 'J', 61: 'S', 62: 'n', 63: 's', 64: '?', 65: 'ü', 66: 'a', 67: 'e', 68: 'ß', 69: 'x', 70: 'z', 71: \"'\"}\n"
     ]
    }
   ],
   "source": [
    "#file_name = '/home/key/Downloads/linux-4.9-rc7/kernel/all.c'\n",
    "file_name = 'faust1.txt'\n",
    "#file_name = 'racine.txt'\n",
    "#file_name = 'mickiewicz.txt'\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    vocab = set(raw_data)\n",
    "    \n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "print(raw_data[:100])\n",
    "print(vocab)\n",
    "print(idx_to_vocab)\n",
    "del raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.49498779166\n",
      "Average training loss for Epoch 1 : 3.19677025197\n",
      "Average training loss for Epoch 2 : 2.80245445289\n",
      "Average training loss for Epoch 3 : 2.46956618159\n",
      "Average training loss for Epoch 4 : 2.28273678293\n",
      "Average training loss for Epoch 5 : 2.16137563481\n",
      "Average training loss for Epoch 6 : 2.06677500874\n",
      "It took 1050.9882757663727 seconds to train for 20 epochs.\n",
      "The average loss on the final epoch was: 2.06677500874\n"
     ]
    }
   ],
   "source": [
    "#save = 'models/linux_GRU_512_120'\n",
    "save = 'models/faust_GRU_512_120'\n",
    "#save = 'models/racine_GRU_256_120'\n",
    "#save = 'models/mickiewicz_GRU_256_120'\n",
    "g1 = build_graph(num_classes=vocab_size, cell_type='GRU', num_steps=120)\n",
    "t = time.time()\n",
    "losses = train_network(g1, num_epochs=7, num_steps=120, save=save)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArbRolSb-ugVtesutt, geun melneg, FistPaichwanst Ach Sein.\n",
      "  ME\n",
      "  Arpihen is meschllt,\n",
      "  Lot do die Mahut Babe\n",
      "  Gehrender Meis eichte finem.\n",
      "  Verweinst de (iuss, wiekel kicheren,\n",
      "  Mopf ast Sas srilbem Iien Euch zurenchls Ktacht acher der at Saschen?\n",
      "  Mas deam Es, Nef ist dau aptenz, sam bustesllacht hlerer Bre, weiese,\n",
      "  Schrast der heines, dieweine:;\n",
      "  Us min mir, veibe de singerb, szemson, web Mieb Auhl,\n",
      "  Aid iie schniesist ieden nlauchegen.  mas- want die icht dieh roh!\n",
      "\n",
      "  FERE: HWast döstean zan Hertohl ich !\n",
      "\n",
      "  FEUHHE:\n",
      "  Ain die Mock Gerenahaz ugn: Har Tegte,,\n",
      "  Eien sicher.\n",
      "\n",
      "  WAr ust köbeb Klügicht gewecht ich,  nam wir mlurtter Sund gähne Mes!  Bich jeben icch.\n",
      "  Uud Kienz, eit zen nicht fan RfustgFricht uun\n",
      "  Ko?  Don Rarte Rärr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g3 = build_graph(num_classes=vocab_size, cell_type='GRU', num_steps=1, batch_size=1)\n",
    "prompt = 'A'\n",
    "generate_characters(g3, save, 750, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
