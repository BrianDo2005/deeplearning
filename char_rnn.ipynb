{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = 120, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    num_classes,\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 128,\n",
    "    batch_size = 32,\n",
    "    num_steps = 120,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln(tensor, scope = None, epsilon = 1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape()) == 2)\n",
    "    m, v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = ''\n",
    "    with tf.variable_scope(scope + 'layer_norm'):\n",
    "        scale = tf.get_variable('scale',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(1))\n",
    "        shift = tf.get_variable('shift',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(0))\n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "\n",
    "    return LN_initial * scale + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalizedLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicLSTMCell to use Layer Normalization.\n",
    "    Note that state_is_tuple is always True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            c, h = state\n",
    "\n",
    "            # change bias argument to False since LN will add bias via shift\n",
    "            concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "            i, j, f, o = tf.split(1, 4, concat)\n",
    "\n",
    "            # add layer normalization to each gate\n",
    "            i = ln(i, scope = 'i/')\n",
    "            j = ln(j, scope = 'j/')\n",
    "            f = ln(f, scope = 'f/')\n",
    "            o = ln(o, scope = 'o/')\n",
    "\n",
    "            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "            # add layer_normalization in calculation of new hidden state\n",
    "            new_h = self._activation(ln(new_c, scope = 'new_h/')) * tf.nn.sigmoid(o)\n",
    "            new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Zueignung.\n",
      "\n",
      "  Ihr naht euch wieder, schwankende Gestalten,\n",
      "  Die früh sich einst dem trüben Blick \n",
      "{\"'\", 'z', 'n', 'B', ' ', 'N', 'j', 'Ä', 'C', 'd', 'Ö', 'ß', 'X', 'H', 'v', 'A', 'O', 'W', 'e', 'I', 'T', 'J', 's', '!', 'F', '.', 'a', ';', 'Z', 'K', 'P', 'Ü', 't', 'G', 'i', 'f', 'q', 'M', '?', 'U', 'b', 'Q', 'l', 'ö', '\"', 'c', 'ü', 'y', 'R', '(', 'h', 'S', '\\n', ',', 'D', 'Y', 'w', 'k', 'g', 'ä', 'o', 'p', '-', 'V', 'm', 'E', 'r', 'L', 'u', ':', 'x', ')'}\n",
      "{0: \"'\", 1: 'z', 2: 'n', 3: 'B', 4: ' ', 5: 'N', 6: 'j', 7: 'Ä', 8: 'C', 9: 'd', 10: 'Ö', 11: 'ß', 12: 'X', 13: 'H', 14: 'v', 15: 'A', 16: 'O', 17: 'W', 18: 'e', 19: 'I', 20: 'T', 21: 'J', 22: 's', 23: '!', 24: 'F', 25: '.', 26: 'a', 27: ';', 28: 'Z', 29: 'K', 30: 'P', 31: 'Ü', 32: 't', 33: 'G', 34: 'i', 35: 'f', 36: 'q', 37: 'M', 38: '?', 39: 'U', 40: 'b', 41: 'Q', 42: 'l', 43: 'ö', 44: '\"', 45: 'c', 46: 'ü', 47: 'y', 48: 'R', 49: '(', 50: 'h', 51: 'S', 52: '\\n', 53: ',', 54: 'D', 55: 'Y', 56: 'w', 57: 'k', 58: 'g', 59: 'ä', 60: 'o', 61: 'p', 62: '-', 63: 'V', 64: 'm', 65: 'E', 66: 'r', 67: 'L', 68: 'u', 69: ':', 70: 'x', 71: ')'}\n"
     ]
    }
   ],
   "source": [
    "#file_name = '/home/key/Downloads/linux-4.9-rc7/kernel/all.c'\n",
    "file_name = 'faust1.txt'\n",
    "#file_name = 'racine.txt'\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    vocab = set(raw_data)\n",
    "    \n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "print(raw_data[:100])\n",
    "print(vocab)\n",
    "print(idx_to_vocab)\n",
    "del raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.88722548765\n",
      "Average training loss for Epoch 1 : 3.33839411362\n",
      "Average training loss for Epoch 2 : 3.28392141473\n",
      "Average training loss for Epoch 3 : 3.26528645497\n",
      "Average training loss for Epoch 4 : 3.20558964505\n",
      "Average training loss for Epoch 5 : 3.06029105186\n",
      "Average training loss for Epoch 6 : 2.91993340324\n",
      "Average training loss for Epoch 7 : 2.80187169243\n",
      "Average training loss for Epoch 8 : 2.70883094096\n",
      "Average training loss for Epoch 9 : 2.63630421489\n",
      "Average training loss for Epoch 10 : 2.58031235022\n",
      "Average training loss for Epoch 11 : 2.53191724478\n",
      "Average training loss for Epoch 12 : 2.48458455591\n",
      "Average training loss for Epoch 13 : 2.43621498463\n",
      "Average training loss for Epoch 14 : 2.38924817478\n",
      "Average training loss for Epoch 15 : 2.34514734792\n",
      "Average training loss for Epoch 16 : 2.30421267771\n",
      "Average training loss for Epoch 17 : 2.26646601453\n",
      "Average training loss for Epoch 18 : 2.23204093353\n",
      "Average training loss for Epoch 19 : 2.20126848127\n",
      "It took 316.76622462272644 seconds to train for 20 epochs.\n",
      "The average loss on the final epoch was: 2.20126848127\n"
     ]
    }
   ],
   "source": [
    "#save = 'models/linux_GRU_128_120'\n",
    "save = 'models/faust_GRU_128_120'\n",
    "#save = 'models/racine_GRU_128_120'\n",
    "#save = 'models/mickiewicz_GRU_128_120'\n",
    "g1 = build_graph(num_classes=vocab_size, cell_type='GRU', num_steps=120)\n",
    "t = time.time()\n",
    "losses = train_network(g1, 20, num_steps=120, save=save)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ng = build_graph(cell_type=\\'LSTM\\', num_steps=80)\\nt = time.time()\\nlosses = train_network(g, 20, num_steps=80, save=\"models/LSTM_20_epochs\")\\nprint(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\\nprint(\"The average loss on the final epoch was:\", losses[-1])\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "g = build_graph(cell_type='LSTM', num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"models/LSTM_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AH(y;'eänünganädes\n",
      "  \n",
      "\n",
      "  AE dohr as gürderdchf urd.\n",
      "  U\"M usbn oÜ shus das it ur Meie\n",
      "\n",
      "  DEH dan asä lerwsam zus vuh LecbenE Vaen Lenimen,\n",
      "  Mige eilt, Bihg mirt veihchtz,\n",
      "  Wof doß ,nstu!  (ishen, such und,\n",
      "  Dise wach sseirt,\n",
      "  Doit der Söens mich ech btelmtuna,\n",
      "  Dut ainen Belebe,\n",
      "  Nür lr zu gite.\n",
      "  Ich Miedoht gbehnlel iu nos soch ct)en\n",
      "\n",
      "  EZTE:\n",
      "  Des wont uct za eßfen,  woa ia fnich eshm zurtr Wehne,\n",
      "  SnÖ Jemneswvion sngster sarneiche!\n",
      "  Ziepe geKdolz teber deü em Lam!\n",
      "  RGain nes Bie zö.  Feie ghelnen ar;\n",
      "  Mach ikchter doahun egc eilen\n",
      "  Kahinur iln hiich ihl Lirmfer,\n",
      "  Dler do mer'.\n",
      "  ÜYe eelguchllen\n",
      "  kEk ein dü buslen! BI dön echm!  Aiumse rain ermessrgehrn.\n",
      "  (Aein sübar wone beh su, bichten gmehd;\n",
      "  daüh zche ird dae.\n",
      "  Ich fai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g3 = build_graph(num_classes=vocab_size, cell_type='GRU', num_steps=1, batch_size=1)\n",
    "prompt = 'A'\n",
    "generate_characters(g3, save, 750, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
