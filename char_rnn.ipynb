{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = 120, batch_size = 32, verbose = True, checkpoint_file=None):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                    g['final_state'],\n",
    "                                                    g['train_step']],\n",
    "                                                    feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "                    \n",
    "        if checkpoint_file is not None:\n",
    "            print(\"Saving variables to '%s'.\" % checkpoint_file)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, checkpoint_file)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    num_classes,\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 256,\n",
    "    batch_size = 32,\n",
    "    num_steps = 120,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint_file, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())        \n",
    "        saver = tf.train.Saver()\n",
    "        print(\"Loading variables from '%s'.\\n\\n\" % checkpoint_file)\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "         \n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Zueignung.\n",
      "\n",
      "  Ihr naht euch wieder, schwankende Gestalten,\n",
      "  Die früh sich einst dem trüben Blick \n",
      "{'G', 'E', 'z', 'P', 'Z', 'O', 'r', 'l', 'ä', 'g', '?', 'Ü', 'J', 'L', 'm', 'U', 'y', 'a', 'Y', ';', '-', 'q', ' ', 'D', 'ß', 'ü', \"'\", 'B', 'F', 'Q', 'X', 'i', '\"', 'M', 'x', 'W', 'C', 'h', 'V', 'o', 'c', 'b', 'w', 'T', 'k', 'N', 'p', 'A', ')', 'K', 'd', '!', ',', 'v', 'Ä', 'f', '(', '.', '\\n', ':', 'H', 'I', 't', 's', 'R', 'j', 'n', 'Ö', 'u', 'ö', 'S', 'e'}\n",
      "{0: 'G', 1: 'E', 2: 'z', 3: 'P', 4: 'Z', 5: 'O', 6: 'r', 7: 'l', 8: 'ä', 9: 'g', 10: '?', 11: 'Ü', 12: 'J', 13: 'L', 14: 'm', 15: 'U', 16: 'y', 17: 'a', 18: 'Y', 19: ';', 20: '-', 21: 'q', 22: ' ', 23: 'D', 24: 'ß', 25: 'ü', 26: \"'\", 27: 'B', 28: 'F', 29: 'Q', 30: 'X', 31: 'i', 32: '\"', 33: 'M', 34: 'x', 35: 'W', 36: 'C', 37: 'h', 38: 'V', 39: 'o', 40: 'c', 41: 'b', 42: 'w', 43: 'T', 44: 'k', 45: 'N', 46: 'p', 47: 'A', 48: ')', 49: 'K', 50: 'd', 51: '!', 52: ',', 53: 'v', 54: 'Ä', 55: 'f', 56: '(', 57: '.', 58: '\\n', 59: ':', 60: 'H', 61: 'I', 62: 't', 63: 's', 64: 'R', 65: 'j', 66: 'n', 67: 'Ö', 68: 'u', 69: 'ö', 70: 'S', 71: 'e'}\n"
     ]
    }
   ],
   "source": [
    "#file_name = '/home/key/Downloads/linux-4.9-rc7/kernel/all.c'\n",
    "file_name = 'faust1.txt'\n",
    "#file_name = 'racine.txt'\n",
    "#file_name = 'mickiewicz.txt'\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    vocab = set(raw_data)\n",
    "    \n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "print(raw_data[:100])\n",
    "print(vocab)\n",
    "print(idx_to_vocab)\n",
    "del raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save = 'models/linux_GRU_512_120'\n",
    "checkpoint_file = 'models/faust_GRU_256_120'\n",
    "#save = 'models/racine_GRU_256_120'\n",
    "#save = 'models/mickiewicz_GRU_256_120'\n",
    "\n",
    "state_size = 256\n",
    "num_epochs = 40\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.67351406696\n",
      "Average training loss for Epoch 1 : 3.27894109838\n",
      "Average training loss for Epoch 2 : 3.21779212297\n",
      "Average training loss for Epoch 3 : 2.94823598862\n",
      "Average training loss for Epoch 4 : 2.70722702438\n",
      "Average training loss for Epoch 5 : 2.55438814444\n",
      "Average training loss for Epoch 6 : 2.4260459554\n",
      "Average training loss for Epoch 7 : 2.32698331627\n",
      "Average training loss for Epoch 8 : 2.25762816971\n",
      "Average training loss for Epoch 9 : 2.19821273579\n",
      "Average training loss for Epoch 10 : 2.1454027447\n",
      "Average training loss for Epoch 11 : 2.09950004839\n",
      "Average training loss for Epoch 12 : 2.06024112655\n",
      "Average training loss for Epoch 13 : 2.02631763383\n",
      "Average training loss for Epoch 14 : 1.99621819047\n",
      "Average training loss for Epoch 15 : 1.96872502682\n",
      "Average training loss for Epoch 16 : 1.94318868132\n",
      "Average training loss for Epoch 17 : 1.91924949721\n",
      "Average training loss for Epoch 18 : 1.8967206221\n",
      "Average training loss for Epoch 19 : 1.87550776846\n",
      "Average training loss for Epoch 20 : 1.85552006609\n",
      "Average training loss for Epoch 21 : 1.83666133179\n",
      "Average training loss for Epoch 22 : 1.81879873837\n",
      "Average training loss for Epoch 23 : 1.80180048008\n",
      "Average training loss for Epoch 24 : 1.78554856777\n",
      "Average training loss for Epoch 25 : 1.77001389803\n",
      "Average training loss for Epoch 26 : 1.7551723181\n",
      "Average training loss for Epoch 27 : 1.74101267375\n",
      "Average training loss for Epoch 28 : 1.72751565073\n",
      "Average training loss for Epoch 29 : 1.71463039342\n",
      "Average training loss for Epoch 30 : 1.70230405705\n",
      "Average training loss for Epoch 31 : 1.69050331443\n",
      "Average training loss for Epoch 32 : 1.67920531946\n",
      "Average training loss for Epoch 33 : 1.66836822734\n",
      "Average training loss for Epoch 34 : 1.65795351945\n",
      "Average training loss for Epoch 35 : 1.64792956792\n",
      "Average training loss for Epoch 36 : 1.63826735814\n",
      "Average training loss for Epoch 37 : 1.62893487659\n",
      "Average training loss for Epoch 38 : 1.61989482478\n",
      "Average training loss for Epoch 39 : 1.61111307612\n",
      "Saving variables to 'models/faust_GRU_256_120'.\n",
      "It took 1966.156409740448 seconds to train the network.\n",
      "The average loss on the final epoch was: 1.61111307612\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    g1 = build_graph(num_classes=vocab_size, state_size = state_size, cell_type='GRU', num_steps=120)\n",
    "    t = time.time()\n",
    "    losses = train_network(g1, num_epochs=num_epochs, num_steps=120, checkpoint_file=checkpoint_file)\n",
    "    print(\"It took\", time.time() - t, \"seconds to train the network.\")\n",
    "    print(\"The average loss on the final epoch was:\", losses[-1])\n",
    "    losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variables from 'models/faust_GRU_256_120'.\n",
      "\n",
      "\n",
      "Cx,F,q H(ErYesen du werben,\n",
      "  Ande Fluch vergreuen,\n",
      "  Baß zang der Manze nur dößes.\n",
      "\n",
      "  BELBER:\n",
      "  Ich so ist'sdas nicht lein!  sü kchpie'n daschen flägen,\n",
      "  Wenn es weure euch in der Linkost.\n",
      "\n",
      "\n",
      "  ELOr einer schner;\n",
      "  Dres Ermoch mir verkund nicht der Schänund bebören,\n",
      "  Siaf alles hott ebagon,\n",
      "  Düßtroß von einem Trofstreistarut.,\n",
      "  Was Gand aber eahn,\n",
      "  Der alö doren auch das Wenn!\n",
      "  Zu hän bat von Fweibeud?\n",
      "\n",
      "  Want.  aus)ejeusel auch.\n",
      "\n",
      "  DIENDE (Wie Wein das warde but?\n",
      "  Und du kerd sie gebagen!\n",
      "  Daßehsorftern weicht Haus mich vor meinen Gracben.\n",
      "\n",
      "  MEPHISTOPHELES:\n",
      "  Gebst doch spfeilie vor.\n",
      "\n",
      "  MIRHER:\n",
      "  Gerauch verkolbstördes Glohche Prarz zu sipben-\n",
      "  Nacht mefßes Verteug schönen Tard das wommest?\n",
      "  Zust im wirqeine Kähre wie.\n",
      "\n",
      "  FAUST:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = build_graph(num_classes=vocab_size, cell_type='GRU', num_steps=1, batch_size=1)\n",
    "prompt = 'C'\n",
    "generate_characters(g2, checkpoint_file, 750, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
