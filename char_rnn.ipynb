{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n",
      " *\n",
      " * This program is free software; you ca\n",
      "{'j', 'w', 'D', '$', '_', '0', '>', 'l', ' ', 'c', 'å', ':', 'q', 'F', 'v', ',', '5', 'J', '=', 'T', 'B', '8', '&', 'S', '9', ';', '[', 'a', '<', 'm', '!', '`', '4', 'U', 'G', '~', '6', 'R', 'X', 'A', 'Z', 'h', '\\n', '?', '^', 'E', 'f', 'H', '*', 'b', 'z', 'n', 'Q', 'r', 'e', 'o', 'x', '+', '#', 'g', '%', '.', '(', 'y', 'L', 'O', '}', '3', '-', '1', 'p', 'P', 'V', \"'\", '\\t', '\"', '\\\\', 'W', 'k', '{', '@', 'N', 'Y', 'I', '|', ')', '/', '7', 'M', 'u', 'C', 'd', ']', 'i', '2', 'K', 't', 's'}\n",
      "{0: 'j', 1: 'w', 2: 'D', 3: '$', 4: '_', 5: '0', 6: '>', 7: 'l', 8: ' ', 9: 'c', 10: 'å', 11: ':', 12: 'q', 13: 'F', 14: 'v', 15: ',', 16: '5', 17: 'J', 18: '=', 19: 'T', 20: 'B', 21: '8', 22: '&', 23: 'S', 24: '9', 25: ';', 26: '[', 27: 'a', 28: '<', 29: 'm', 30: '!', 31: '`', 32: '4', 33: 'U', 34: 'G', 35: '~', 36: '6', 37: 'R', 38: 'X', 39: 'A', 40: 'Z', 41: 'h', 42: '\\n', 43: '?', 44: '^', 45: 'E', 46: 'f', 47: 'H', 48: '*', 49: 'b', 50: 'z', 51: 'n', 52: 'Q', 53: 'r', 54: 'e', 55: 'o', 56: 'x', 57: '+', 58: '#', 59: 'g', 60: '%', 61: '.', 62: '(', 63: 'y', 64: 'L', 65: 'O', 66: '}', 67: '3', 68: '-', 69: '1', 70: 'p', 71: 'P', 72: 'V', 73: \"'\", 74: '\\t', 75: '\"', 76: '\\\\', 77: 'W', 78: 'k', 79: '{', 80: '@', 81: 'N', 82: 'Y', 83: 'I', 84: '|', 85: ')', 86: '/', 87: '7', 88: 'M', 89: 'u', 90: 'C', 91: 'd', 92: ']', 93: 'i', 94: '2', 95: 'K', 96: 't', 97: 's'}\n"
     ]
    }
   ],
   "source": [
    "file_name = '/home/key/Downloads/linux-4.9-rc7/kernel/all.c'\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    vocab = set(raw_data)\n",
    "    \n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "print(raw_data[:100])\n",
    "print(vocab)\n",
    "print(idx_to_vocab)\n",
    "del raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = 120, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 128,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 120,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln(tensor, scope = None, epsilon = 1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape()) == 2)\n",
    "    m, v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = ''\n",
    "    with tf.variable_scope(scope + 'layer_norm'):\n",
    "        scale = tf.get_variable('scale',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(1))\n",
    "        shift = tf.get_variable('shift',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(0))\n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "\n",
    "    return LN_initial * scale + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalizedLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicLSTMCell to use Layer Normalization.\n",
    "    Note that state_is_tuple is always True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            c, h = state\n",
    "\n",
    "            # change bias argument to False since LN will add bias via shift\n",
    "            concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "            i, j, f, o = tf.split(1, 4, concat)\n",
    "\n",
    "            # add layer normalization to each gate\n",
    "            i = ln(i, scope = 'i/')\n",
    "            j = ln(j, scope = 'j/')\n",
    "            f = ln(f, scope = 'f/')\n",
    "            o = ln(o, scope = 'o/')\n",
    "\n",
    "            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "            # add layer_normalization in calculation of new hidden state\n",
    "            new_h = self._activation(ln(new_c, scope = 'new_h/')) * tf.nn.sigmoid(o)\n",
    "            new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.10970336368\n",
      "Average training loss for Epoch 1 : 2.34120994661\n",
      "Average training loss for Epoch 2 : 2.02586186176\n",
      "Average training loss for Epoch 3 : 1.83284370382\n",
      "Average training loss for Epoch 4 : 1.70492588829\n",
      "Average training loss for Epoch 5 : 1.6128305805\n",
      "Average training loss for Epoch 6 : 1.54402266864\n",
      "Average training loss for Epoch 7 : 1.49071158275\n",
      "Average training loss for Epoch 8 : 1.44772524777\n",
      "Average training loss for Epoch 9 : 1.41203714179\n",
      "Average training loss for Epoch 10 : 1.38177386954\n",
      "Average training loss for Epoch 11 : 1.35571193106\n",
      "Average training loss for Epoch 12 : 1.33297513603\n",
      "Average training loss for Epoch 13 : 1.31288076414\n",
      "Average training loss for Epoch 14 : 1.2949077397\n",
      "Average training loss for Epoch 15 : 1.27867760804\n",
      "Average training loss for Epoch 16 : 1.26390564819\n",
      "Average training loss for Epoch 17 : 1.25036272306\n",
      "Average training loss for Epoch 18 : 1.23785472765\n",
      "Average training loss for Epoch 19 : 1.22622996871\n",
      "It took 7768.347198486328 seconds to train for 20 epochs.\n",
      "The average loss on the final epoch was: 1.22622996871\n"
     ]
    }
   ],
   "source": [
    "g1 = build_graph(cell_type='GRU', num_steps=120)\n",
    "t = time.time()\n",
    "losses = train_network(g1, 20, num_steps=120, save=\"models/GRU_128_120\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ng = build_graph(cell_type=\\'LSTM\\', num_steps=80)\\nt = time.time()\\nlosses = train_network(g, 20, num_steps=80, save=\"models/LSTM_20_epochs\")\\nprint(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\\nprint(\"The average loss on the final epoch was:\", losses[-1])\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "g = build_graph(cell_type='LSTM', num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"models/LSTM_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/QSOWANTBULTU_TRRYRASH_SYSZP]\n",
      "\t */\n",
      "\twrite_buffer_entries_lock();\n",
      "\n",
      "\tif (audit->load->flags & cmp_mad ||\n",
      "\t    tathcomp_finishet_put(sec, dop, next)) {\n",
      "\t\ttest:\t\t\t\t\t      profile->perf_all && nr_call->init, 0);\n",
      "\t\tcurrent->lock = 0; i < so = and;\n",
      "\tcase TIDE::\n",
      "\t\tret = sdltxtrab(0);\n",
      "\t\tcount = info->hb_to_ns[ivers);\n",
      "\t\tif (pd_threads) {\n",
      "\t\t\tfor_to_next_off(comproup->curr_ecps);\n",
      "\t\t\tif (_j->cpu_szaute <= NULL ||\n",
      "\t\t\t * info fields[iter->function.dlistvalls_current) {\n",
      "\t\t\t\traw_read = wait_log_can_err_free_cpu(vmop_new_log_lal, *ullaglec\", 3duwnabacktrizer);\n",
      "\t\t\t\tif (p->next_t) {\n",
      "\t\t\t\t\tif (!(info->per_cpumask)\n",
      "\t\t\t\t\tcase freeting_tasks.handler[PM_SIQDE);\n",
      "\t\t\t\tconst bsesscture_resource;\n",
      "\t\t\t\tcontinue2_printk(\"vorkering.tvstark }tmap %s slccmp, *intersnon *offset,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g3 = build_graph(cell_type='GRU', num_steps=1, batch_size=1)\n",
    "generate_characters(g3, \"models/GRU_128_120\", 750, prompt='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
